{"cells":[{"cell_type":"markdown","source":["# Imports"],"metadata":{"id":"vy7LcUPlsK0X"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"FbWvCAKmUQD6"},"outputs":[],"source":["import sys\n","import os\n","from google.colab import drive\n","from google.colab import files\n","from dotenv import load_dotenv\n","import json\n","import warnings\n","import pickle\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","warnings.filterwarnings(\"ignore\")\n","%matplotlib inline"]},{"cell_type":"markdown","source":["# Bootstrap"],"metadata":{"id":"zdP_xX80sN98"}},{"cell_type":"code","source":["np.random.seed(31071967)\n","\n","# Find and load the .env file from the current or parent directories\n","load_dotenv()\n","\n","drive.mount('/content/drive')\n","\n","with open(f\"{os.getenv('PROJECT_PATH')}/src/config.json\", 'r') as f:\n","    project_config = json.load(f)\n","    project_config.pop('_comment', None)\n","    project_config.pop('_note', None)\n","    f.close()"],"metadata":{"id":"jsWS0uXZwymE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# sanity check\n","project_config[\"TKL\"]"],"metadata":{"id":"X_dsR2Ov4Zk5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Download TKL data from YF"],"metadata":{"id":"JcreNdJF7kFy"}},{"cell_type":"code","source":["tickers_yf = {\n","    \"y\"     : f\"{project_config['TKL']}\",\n","    \"NASDAQ\"      : \"^IXIC\",\n","    \"SP500\"       : \"^GSPC\",\n","    \"RealEstate\"  : \"VNQ\",\n","    \"Oil_WTI\"     : \"CL=F\",\n","    \"Gold\"        : \"GC=F\",\n","    \"Inflation_Exp\": \"^TNX\"\n","}"],"metadata":{"id":"Nsr0rFkraP2M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import yfinance as yf\n","import pandas as pd\n","from pandas_datareader import data as pdr\n","\n","from datetime import date, timedelta\n","end_date = date.today() - timedelta(days=1)\n","start_date = end_date - timedelta(days=int(project_config[\"HISTORY_DEPTH\"]))\n","\n","# ---- DOWNLOAD FROM YAHOO FINANCE ----\n","ts_yf = yf.download(\n","    tickers=list(tickers_yf.values()),\n","    start=start_date,\n","    end=end_date,\n","    auto_adjust=True\n",")[\"Close\"]\n","\n","# rename columns to readable names\n","rename_map = {v: k for k, v in tickers_yf.items()}\n","ts_yf = ts_yf.rename(columns=rename_map)\n","#ts_yf = ts_yf.drop(columns=[\"Ticker\"])\n","\n","# Fill missing daily values for macro data (monthly)\n","ts_yf = ts_yf.ffill()   # forward fill\n","ts_yf = ts_yf.dropna()  # drop any remaining rows\n","\n","ts_yf = ts_yf.reset_index().rename(columns={\"Date\": \"Date\"})\n","\n","print(f\"\\n\\nDataset for y={project_config['TKL']}\")\n","display(ts_yf.head(1))\n","display(ts_yf.tail(1))\n","ts_yf.info()"],"metadata":{"id":"kbRfMRPGqtDO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Turn time series into supprvied learning table"],"metadata":{"id":"k8orD4C3YpKN"}},{"cell_type":"code","source":["df = ts_yf.copy()\n","\n","lags =   [2,5,10,22,65] # lookback days\n","windows =  [5,10,22,65] # statistics windows\n","\n","# Identify columns to generate features for (excluding 'index' column which is the date)\n","feature_base_cols = [c for c in df.columns if c not in ['Date','index']]\n","\n","for base_col in feature_base_cols:\n","  for lag in lags:\n","    df[f\"{base_col}_lag_{lag}\"] = df[base_col].shift(lag)\n","\n","  # Generate rolling window statistics for the current base_col\n","  for window in windows:\n","    df[f\"{base_col}_min_{window}\"] = df[base_col].rolling(window=window).min()\n","    df[f\"{base_col}_max_{window}\"] = df[base_col].rolling(window=window).max()\n","    df[f\"{base_col}_mean_{window}\"] = df[base_col].rolling(window=window).mean()\n","    df[f\"{base_col}_std_{window}\"]  = df[base_col].rolling(window=window).std()\n","    df[f\"{base_col}_diff_{window}\"] = df[base_col].diff(window)\n","    df[f\"{base_col}_pct_{window}\"] = df[base_col].pct_change(window)\n","\n","\n","df['y_next'] = df['y'].shift(-1)    # tomorrow's close price\n","df.tail(1)['y_next'] = -1           # the TARGET cell\n","\n","df.dropna(inplace=True)\n","\n","from sklearn.preprocessing import MinMaxScaler\n","scaler = MinMaxScaler()\n","\n","cols_to_scale = df.drop(['Date', 'y_next'], axis=1).columns\n","\n","for col in cols_to_scale:\n","  df[f\"{col}_orig\"] = df[col]\n","\n","df[cols_to_scale] = scaler.fit_transform(df[cols_to_scale])\n","\n","df.tail(1)"],"metadata":{"id":"EvmwqsAoXG-b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Split"],"metadata":{"id":"Uzq7ct1kay6t"}},{"cell_type":"code","source":["tommorw_date = df.tail(1)['Date'] # Changed 'Date' to 'index'\n","\n","X = df[cols_to_scale]\n","y = df['y_next']\n","\n","X_train, X_test = X.iloc[:-2], X.iloc[-1:]\n","y_train, y_test = y.iloc[:-2], y.iloc[-1:]\n","\n","X_train.shape, X_test.shape, y_train.shape, y_test.shape"],"metadata":{"id":"Yv7zAuvAaxQK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Train, Predict, Evaluate"],"metadata":{"id":"P2Oqmkp-mI87"}},{"cell_type":"code","source":["from xgboost import XGBRegressor\n","from sklearn.metrics import mean_absolute_error\n","\n","xgb = XGBRegressor(\n","    n_estimators=300,\n","    max_depth=5,\n","    learning_rate=0.05,\n","    subsample=0.8,\n","    colsample_bytree=0.8,\n","    objective=\"reg:squarederror\")\n","\n","xgb.fit(X_train, y_train)"],"metadata":{"id":"HoYw9Xi9mASg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pred = xgb.predict(X_test)\n","mae = mean_absolute_error(y_test, pred)\n","print(\"MAE:\", mae)"],"metadata":{"id":"EnRMoVhanbkD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["last_row = X_test # Use X_test which was successfully predicted upon\n","tomorrow_prediction = xgb.predict(last_row)[0]\n","\n","print(f\"Stoke: {project_config['TKL']}\")\n","print(f\"Date: {tommorw_date.iloc[0].date()}\")\n","print(f\"Close Prediction: ${tomorrow_prediction:.2f}\")"],"metadata":{"id":"UTloMMYAnTNp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Feature selection"],"metadata":{"id":"GBZeXSO_rQqv"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import xgboost # Import the xgboost module\n","\n","# Plot feature importance based on \"weight\" (number of times a feature appears in a tree)\n","plt.figure(figsize=(10, 6))\n","xgboost.plot_importance(xgb, max_num_features=20) # Pass the xgb regressor object\n","plt.title(\"Feature Importance (Weight)\")\n","plt.show()\n","\n","# Get the feature importance dictionary\n","# importance_type can be 'weight', 'gain', or 'cover'\n","importance_dict = xgb.get_booster().get_score(importance_type='weight')\n","\n","# 2. Convert to a Pandas DataFrame for easy sorting and viewing\n","df_importance = pd.DataFrame(list(importance_dict.items()), columns=['Feature', 'Score'])\n","\n","# 3. Sort by Score (High to Low)\n","df_importance = df_importance.sort_values(by='Score', ascending=False)\n","\n","# 4. Extract just the names into a Python list (for use in code)\n","top_weight_features_list = df_importance['Feature'].head(12).tolist()\n","print(top_weight_features_list)"],"metadata":{"id":"XyRoLkmynur4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# OPTIONAL: Plot based on \"gain\" (average gain of splits which use the feature)\n","# \"Gain\" is often more accurate for finding what actually drives the prediction.\n","plt.figure(figsize=(10, 6))\n","xgboost.plot_importance(xgb, importance_type='gain', max_num_features=20) # Pass the xgb regressor object\n","plt.title(\"Feature Importance (Gain)\")\n","plt.show()\n","\n","# Get the feature importance dictionary\n","# importance_type can be 'weight', 'gain', or 'cover'\n","importance_dict = xgb.get_booster().get_score(importance_type='gain')\n","\n","# 2. Convert to a Pandas DataFrame for easy sorting and viewing\n","df_importance = pd.DataFrame(list(importance_dict.items()), columns=['Feature', 'Score'])\n","\n","# 3. Sort by Score (High to Low)\n","df_importance = df_importance.sort_values(by='Score', ascending=False)\n","\n","# 4. Extract just the names into a Python list (for use in code)\n","top_gain_features_list = df_importance['Feature'].head(12).tolist()\n","print(top_gain_features_list)"],"metadata":{"id":"A6iSN9ORDNl9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["top_features = list(set(top_gain_features_list) | set(top_weight_features_list))\n","top_features"],"metadata":{"id":"QFXccHsOFtaf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Final df"],"metadata":{"id":"aNvZPouorV0Q"}},{"cell_type":"code","source":["final_df = df[['Date','y_next'] + top_features]\n","\n","for col in top_features:\n","  final_df[f\"{col}_orig\"] = df[f\"{col}_orig\"]\n","\n","display(final_df.tail(1))\n","final_df.info()"],"metadata":{"id":"Z3VS6_y9K-9B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# CSV"],"metadata":{"id":"-Rl3qzjtraC6"}},{"cell_type":"code","source":["df_csv_path = f\"{os.getenv('PROJECT_PATH')}{project_config['data_directory']}{project_config['TKL']}.df.csv\"\n","final_df.to_csv(df_csv_path)"],"metadata":{"id":"eQoUheoTLYV4"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"mount_file_id":"1z_z6aKOseMCo3v_YjFnMhl-K4gXimAMR","authorship_tag":"ABX9TyOaTiMEG+OnkLacC+BeJsj4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
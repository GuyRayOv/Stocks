{"cells":[{"cell_type":"markdown","source":["# Imports"],"metadata":{"id":"vy7LcUPlsK0X"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"FbWvCAKmUQD6"},"outputs":[],"source":["import sys\n","import os\n","from google.colab import drive\n","from google.colab import files\n","from dotenv import load_dotenv\n","import json\n","import warnings\n","import pickle\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","warnings.filterwarnings(\"ignore\")\n","%matplotlib inline"]},{"cell_type":"markdown","source":["# Bootstrap"],"metadata":{"id":"zdP_xX80sN98"}},{"cell_type":"code","source":["np.random.seed(31071967)\n","\n","# Find and load the .env file from the current or parent directories\n","load_dotenv()\n","\n","drive.mount('/content/drive')\n","\n","with open(f\"{os.getenv('PROJECT_PATH')}/src/config.json\", 'r') as f:\n","    project_config = json.load(f)\n","    project_config.pop('_comment', None)\n","    project_config.pop('_note', None)\n","    f.close()"],"metadata":{"id":"jsWS0uXZwymE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Download TKL data from YF"],"metadata":{"id":"JcreNdJF7kFy"}},{"cell_type":"code","source":["tickers_yf = {\n","    \"y\"     : f\"{project_config['TKL']}\",\n","    \"NASDAQ\"       : \"^IXIC\",\n","    \"SP500\"        : \"^GSPC\",\n","    \"Gold\"         : \"GC=F\",\n","    \"Oil\"          : \"CL=F\",\n","    \"RealEstate\"   : \"VNQ\",\n","    \"InflationExp\": \"^TNX\"\n","}\n","\n","desired_order = [\n","    \"Date\",\n","    \"y\",\n","    \"NASDAQ\",\n","    \"SP500\",\n","    \"Oil\",\n","    \"Gold\",\n","    \"RealEstate\",\n","    \"InflationExp\",\n","]"],"metadata":{"id":"Nsr0rFkraP2M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import yfinance as yf\n","import pandas as pd\n","from pandas_datareader import data as pdr\n","\n","from datetime import date, timedelta\n","end_date = date.today() - timedelta(days=1)\n","start_date = end_date - timedelta(days=int(project_config[\"HISTORY_DEPTH\"]))\n","\n","# ---- DOWNLOAD FROM YAHOO FINANCE ----\n","ts_yf = yf.download(\n","    tickers=list(tickers_yf.values()),\n","    start=start_date,\n","    end=end_date,\n","    auto_adjust=True\n",")[\"Close\"]\n","\n","#ts_yf = ts_yf.dropna(how=\"all\")\n","# ts_yf = yf.download(\n","#     tickers=list(tickers_yf.values()),\n","#     period=\"max\",\n","#     auto_adjust=True\n","# )[\"Close\"]\n","#ts_yf = ts_yf.tail(int(project_config[\"HISTORY_DEPTH\"]))\n","\n","# rename columns to readable names\n","rename_map = {v: k for k, v in tickers_yf.items()}\n","ts_yf = ts_yf.rename(columns=rename_map)\n","\n","# Fill missing daily values for macro data (monthly)\n","ts_yf = ts_yf.fillna(method='ffill').fillna(method='bfill')\n","ts_yf = ts_yf.reset_index().rename(columns={\"Date\": \"Date\",})\n","\n","print(f\"\\n\\nDataset for y={project_config['TKL']}\")\n","display(ts_yf.head(1))\n","display(ts_yf.tail(1))\n","ts_yf.info()"],"metadata":{"id":"kbRfMRPGqtDO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Turn time-series into scaled supprvied ML table"],"metadata":{"id":"k8orD4C3YpKN"}},{"cell_type":"code","source":["df = ts_yf.copy()\n","df_orig = ts_yf.copy()\n","\n","# Keep Date\n","date_col = df[\"Date\"]\n","\n","# Targets\n","y_col = ['y']\n","\n","# Features\n","X_cols = df.drop(columns=y_col+['Date']).columns\n","\n","# Initialize scalers\n","from sklearn.preprocessing import MinMaxScaler\n","X_scaler = MinMaxScaler()\n","y_scaler = MinMaxScaler()\n","\n","# Scale\n","df_X_scaled = pd.DataFrame(X_scaler.fit_transform(df[X_cols]),\n","                           columns=X_cols, index=df.index)\n","\n","df_y_scaled = pd.DataFrame(y_scaler.fit_transform(df[y_col]),\n","                           columns=y_col, index=df.index)\n","\n","# Rebuild dataframe\n","df = pd.concat([date_col, df_X_scaled, df_y_scaled], axis=1)\n","\n","df = df[desired_order]\n","df_orig = df_orig[desired_order]\n","\n","del df_X_scaled, df_y_scaled\n","\n","display(df.tail(1))\n","display(df_orig.tail(1))"],"metadata":{"id":"vcW-UFx7L3Vf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_ts_features(df, lags=[2,3,4,5,6,10,22,66], windows=[5,10,22,66]):\n","\n","  # Identify columns to generate features for (excluding 'index' column which is the date)\n","  base_cols = [c for c in df.columns if c not in ['Date','index']]\n","\n","  for base_col in base_cols:\n","\n","    for lag in lags:\n","      df[f\"{base_col}_lag_{lag}\"] = df[base_col].shift(lag)\n","\n","  # Generate rolling window statistics for the current base_col\n","    for window in windows:\n","      df[f\"{base_col}_min_{window}\"] = df[base_col].rolling(window=window).min()\n","      df[f\"{base_col}_max_{window}\"] = df[base_col].rolling(window=window).max()\n","      df[f\"{base_col}_mean_{window}\"] = df[base_col].rolling(window=window).mean()\n","      df[f\"{base_col}_std_{window}\"]  = df[base_col].rolling(window=window).std()\n","      df[f\"{base_col}_diff_{window}\"] = df[base_col].diff(window)\n","      df[f\"{base_col}_pct_{window}\"] = df[base_col].pct_change(window)\n","\n","  df['y_next'] = df['y'].shift(-1)                            # y_next = tomorrow's y (close price)\n","  df.loc[df.index[-1], 'y_next'] = df.loc[df.index[-1], 'y']  # the TARGET cell. y_next tomorrow = y today\n","  df = df.fillna(method='ffill').fillna(method='bfill')\n","\n","  return df\n","\n","df = generate_ts_features(df)\n","df_orig = generate_ts_features(df_orig)\n","\n","# Replace infinite values with NaN in X_train and y_train\n","df = df.replace([np.inf, -np.inf], np.nan)\n","df = df.fillna(method='ffill').fillna(method='bfill')\n","\n","display(df.tail(1))\n","display(df_orig.tail(1))"],"metadata":{"id":"EvmwqsAoXG-b","colab":{"base_uri":"https://localhost:8080/","height":211},"executionInfo":{"status":"error","timestamp":1766000199475,"user_tz":-120,"elapsed":10,"user":{"displayName":"Guy Ray","userId":"03709370811123485399"}},"outputId":"7081ab9e-1d72-4120-dbca-d3f6a94387c9"},"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'df' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2188578746.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_ts_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mdf_orig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_ts_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_orig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"]}]},{"cell_type":"markdown","source":["# Split"],"metadata":{"id":"Uzq7ct1kay6t"}},{"cell_type":"code","source":["X = df.drop(columns=['Date','y_next'])\n","y = df['y_next']\n","\n","X_train, X_test = X.iloc[:-2], X.iloc[-1:]\n","y_train, y_test = y.iloc[:-2], y.iloc[-1:]\n","\n","del X,y\n","\n","X_train.shape, X_test.shape, y_train.shape, y_test.shape"],"metadata":{"id":"Yv7zAuvAaxQK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Train, Predict, Evaluate"],"metadata":{"id":"P2Oqmkp-mI87"}},{"cell_type":"code","source":["from xgboost import XGBRegressor\n","from sklearn.metrics import mean_absolute_error\n","\n","xgb = XGBRegressor(\n","    n_estimators=300,\n","    max_depth=5,\n","    learning_rate=0.05,\n","    subsample=0.8,\n","    colsample_bytree=0.8,\n","    objective=\"reg:squarederror\")\n","\n","xgb.fit(X_train, y_train)"],"metadata":{"id":"HoYw9Xi9mASg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pred = xgb.predict(X_test)\n","mae = mean_absolute_error(y_test, pred)\n","print(\"MAE:\", mae)"],"metadata":{"id":"EnRMoVhanbkD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["last_row = X_test # Use X_test which was successfully predicted upon\n","scaled_tomorrow_prediction = xgb.predict(last_row)[0] # This is a scaled prediction for y_next\n","\n","# Calculate min and max for 'y_next' from the original unscaled data (df_orig)\n","# This allows us to manually inverse transform the single predicted value.\n","min_y_next_orig = df_orig['y_next'].min()\n","max_y_next_orig = df_orig['y_next'].max()\n","\n","# Inverse transform the scaled prediction using the min-max formula\n","unscaled_tomorrow_prediction = scaled_tomorrow_prediction * (max_y_next_orig - min_y_next_orig) + min_y_next_orig\n","\n","# Update the scaled 'y_next' in the scaled DataFrame (df) for consistency if needed later\n","df.loc[df.index[-1], 'y_next'] = scaled_tomorrow_prediction\n","df_orig.loc[df_orig.index[-1], 'y_next'] = unscaled_tomorrow_prediction\n","\n","print(f\"Stoke: {project_config['TKL']}\")\n","print(f\"Date: { df['Date'].tail(1).iloc[0].date()}\")\n","print(f\"Close Prediction: ${unscaled_tomorrow_prediction:.2f}\")"],"metadata":{"id":"UTloMMYAnTNp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["display(df.tail(1))\n","display(df_orig.tail(1))"],"metadata":{"id":"mGA0L4ByeCGo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Feature selection"],"metadata":{"id":"GBZeXSO_rQqv"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import xgboost # Import the xgboost module\n","\n","# Plot feature importance based on \"weight\" (number of times a feature appears in a tree)\n","plt.figure(figsize=(10, 6))\n","xgboost.plot_importance(xgb, max_num_features=20) # Pass the xgb regressor object\n","plt.title(\"Feature Importance (Weight)\")\n","plt.show()\n","\n","# 1. Get feature importance by weight\n","importance_dict = xgb.get_booster().get_score(importance_type='weight')\n","\n","# 2. Convert to DataFrame\n","df_importance = pd.DataFrame(list(importance_dict.items()), columns=['Feature', 'Score'])\n","\n","# 3. Sort high â†’ low\n","df_importance = df_importance.sort_values(by='Score', ascending=False)\n","\n","# 4. Compute threshold = 10% of top feature\n","top_score = df_importance['Score'].iloc[0]\n","threshold = top_score * 0.05   # 5%\n","\n","# 5. Select only strong features\n","df_top = df_importance[df_importance['Score'] >= threshold]\n","\n","# 6. Convert to list\n","top_weight_features_list = df_top['Feature'].tolist()[:10:]\n","\n","print(\"Weight Threshold:\", threshold)\n","print(\"Selected Weight Features:\", top_weight_features_list)"],"metadata":{"id":"XyRoLkmynur4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# OPTIONAL: Plot based on \"gain\" (average gain of splits which use the feature)\n","# \"Gain\" is often more accurate for finding what actually drives the prediction.\n","plt.figure(figsize=(10, 6))\n","xgboost.plot_importance(xgb, importance_type='gain', max_num_features=20) # Pass the xgb regressor object\n","plt.title(\"Feature Importance (Gain)\")\n","plt.show()\n","\n","# 1. Build importance DataFrame\n","importance_dict = xgb.get_booster().get_score(importance_type='gain')\n","df_importance = pd.DataFrame(list(importance_dict.items()), columns=['Feature', 'Score'])\n","\n","# 2. Sort high to low\n","df_importance = df_importance.sort_values(by='Score', ascending=False)\n","\n","# 3. Compute threshold = 10% of top feature\n","top_score = df_importance['Score'].iloc[0]\n","threshold = top_score * 0.05\n","\n","# 4. Select features with Score >= threshold\n","df_top = df_importance[df_importance['Score'] >= threshold]\n","\n","# 5. Extract feature names\n","top_gain_features_list = df_top['Feature'].tolist()[:10:]\n","\n","print(\"Threshold:\", threshold)\n","print(\"Selected features:\", top_gain_features_list)"],"metadata":{"id":"GQ9BBo0dUEJa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["top_features = list(set(top_gain_features_list) | set(top_weight_features_list))\n","top_features"],"metadata":{"id":"QFXccHsOFtaf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Final df for this TKL"],"metadata":{"id":"aNvZPouorV0Q"}},{"cell_type":"code","source":["final_df = df[['Date','y_next'] + top_features]\n","\n","for col in top_features + ['y_next']:\n","  final_df[f\"{col}_orig\"] = df_orig[col]\n","\n","display(final_df.tail(1))\n","final_df.info()"],"metadata":{"id":"Z3VS6_y9K-9B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# CSV"],"metadata":{"id":"-Rl3qzjtraC6"}},{"cell_type":"code","source":["if project_config[\"dataprep.override_csv\"] == '1':\n","\n","  df_csv_path = f\"{os.getenv('PROJECT_PATH')}{project_config['data_directory']}{project_config['TKL']}.df.csv\"\n","  final_df[:-2:].to_csv(df_csv_path)"],"metadata":{"id":"hDmQ7MMhW2mA"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"mount_file_id":"1z_z6aKOseMCo3v_YjFnMhl-K4gXimAMR","authorship_tag":"ABX9TyMqNIYepqwvfgMzrJS1rR+s"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
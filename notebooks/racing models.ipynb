{"cells":[{"cell_type":"markdown","metadata":{"id":"4Ns2SNtAqGwM"},"source":["# Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hjNT5fdaAq2u"},"outputs":[],"source":["import sys\n","import os\n","from google.colab import drive\n","from google.colab import files\n","from dotenv import load_dotenv\n","import json\n","import warnings\n","import pickle\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","warnings.filterwarnings(\"ignore\")\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"DRGyC6FrqbZA"},"source":["# Bootstrap"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gch0QCmZIG5C"},"outputs":[],"source":["np.random.seed(31071967)\n","\n","# Find and load the .env file from the current or parent directories\n","load_dotenv()\n","\n","drive.mount('/content/drive')\n","\n","with open(f\"{os.getenv('PROJECT_PATH')}/src/config.json\", 'r') as f:\n","    project_config = json.load(f)\n","    project_config.pop('_comment', None)\n","    project_config.pop('_note', None)\n","    project_config.pop\n","    f.close()"]},{"cell_type":"code","source":["project_config['STOCKS'] = project_config['STOCKS'].split()\n","project_config['TKL'] = project_config['STOCKS'][0]"],"metadata":{"id":"ZM6NNI3gGtMW"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YY6gc_3UZbUo"},"outputs":[],"source":["if project_config['chain_notebooks'] == '1':\n","\n","  !pip install papermill\n","  !pip install nbconvert\n","  !pip install nbformat\n","  !pip install IPython\n","\n","  import papermill as pm\n","  import nbformat\n","  from nbconvert import HTMLExporter\n","  from IPython.display import HTML, display\n","\n","  input_file = f\"{os.getenv('PROJECT_PATH')}{project_config['notebooks_directory']}{project_config['notebook1']}\"\n","  output_file = f\"{os.getenv('PROJECT_PATH')}{project_config['output_directory']}{project_config['output1']}\"\n","\n","  # --- Execute the proviuse notebook with parameters ---\n","  pm.execute_notebook(\n","      input_path = input_file,\n","      output_path = output_file,\n","      log_output=False,  # don't print logs while running\n","      progress_bar=True\n","  )\n","\n","  # --- Convert the executed notebook to HTML ---\n","  nb = nbformat.read(output_file, as_version=4)\n","  html_exporter = HTMLExporter()\n","  html_exporter.template_name = \"lab\"  # modern look; alternatives: 'classic', 'basic'\n","  body, _ = html_exporter.from_notebook_node(nb)\n","\n","  # --- Display the HTML result inline ---\n","  display(HTML(body))"]},{"cell_type":"code","source":["tickers_yf = {\n","    \"y\"     : f\"{project_config['TKL']}\",\n","    \"NASDAQ\"       : \"^IXIC\",\n","    \"SP500\"        : \"^GSPC\",\n","    \"Gold\"         : \"GC=F\",\n","    \"Oil\"          : \"CL=F\",\n","    \"RealEstate\"   : \"VNQ\",\n","    \"InflationExp\": \"^TNX\"\n","}\n","\n","desired_order = [\n","    \"Date\",\n","    \"y\",\n","    \"NASDAQ\",\n","    \"SP500\",\n","    \"Oil\",\n","    \"Gold\",\n","    \"RealEstate\",\n","    \"InflationExp\",\n","]"],"metadata":{"id":"7LUL6NLy72f5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import yfinance as yf\n","import pandas as pd\n","from pandas_datareader import data as pdr\n","\n","from datetime import date, timedelta\n","end_date = date.today() - timedelta(days=1)\n","start_date = end_date - timedelta(days=int(project_config[\"HISTORY_DEPTH\"]))\n","\n","if project_config['TKL'] == 'TNYA':\n","  start_date = pd.to_datetime(\"31.07.2022\", format=\"%d.%m.%Y\")\n","\n","# ---- DOWNLOAD FROM YAHOO FINANCE ----\n","ts_yf = yf.download(\n","    tickers=list(tickers_yf.values()),\n","    start=start_date,\n","    end=end_date,\n","    auto_adjust=True\n",")[\"Close\"]\n","\n","# rename columns to readable names\n","rename_map = {v: k for k, v in tickers_yf.items()}\n","ts_yf = ts_yf.rename(columns=rename_map)\n","\n","# Fill missing daily values for macro data (monthly)\n","ts_yf = ts_yf.fillna(method='ffill').fillna(method='bfill')\n","ts_yf = ts_yf.reset_index().rename(columns={\"Date\": \"Date\",})\n","\n","print(f\"\\n\\nDataset for y={project_config['TKL']}\")\n","display(ts_yf.head(1))\n","display(ts_yf.tail(1))\n","ts_yf.info()"],"metadata":{"id":"CcGxZwc479c0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = ts_yf.copy()\n","df_orig = ts_yf.copy()\n","\n","# Keep Date\n","date_col = df[\"Date\"]\n","\n","# Targets\n","y_col = ['y']\n","\n","# Features\n","X_cols = df.drop(columns=y_col+['Date']).columns\n","\n","# Initialize scalers\n","from sklearn.preprocessing import MinMaxScaler\n","X_scaler = MinMaxScaler()\n","y_scaler = MinMaxScaler()\n","\n","# Scale\n","df_X_scaled = pd.DataFrame(X_scaler.fit_transform(df[X_cols]),\n","                           columns=X_cols, index=df.index)\n","\n","df_y_scaled = pd.DataFrame(y_scaler.fit_transform(df[y_col]),\n","                           columns=y_col, index=df.index)\n","\n","# Rebuild dataframe\n","df = pd.concat([date_col, df_X_scaled, df_y_scaled], axis=1)\n","\n","df = df[desired_order]\n","df_orig = df_orig[desired_order]\n","\n","del df_X_scaled, df_y_scaled\n","\n","display(df.tail(1))\n","display(df_orig.tail(1))"],"metadata":{"id":"KFYxPL8b8A_s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_ts_features(df, lags=[2,3,4,5,6,10,22,66], windows=[5,10,22,66]):\n","\n","  # Identify columns to generate features for (excluding 'index' column which is the date)\n","  base_cols = [c for c in df.columns if c not in ['Date','index']]\n","\n","  for base_col in base_cols:\n","\n","    for lag in lags:\n","      df[f\"{base_col}_lag_{lag}\"] = df[base_col].shift(lag)\n","\n","  # Generate rolling window statistics for the current base_col\n","    for window in windows:\n","      df[f\"{base_col}_min_{window}\"] = df[base_col].rolling(window=window).min()\n","      df[f\"{base_col}_max_{window}\"] = df[base_col].rolling(window=window).max()\n","      df[f\"{base_col}_mean_{window}\"] = df[base_col].rolling(window=window).mean()\n","      df[f\"{base_col}_std_{window}\"]  = df[base_col].rolling(window=window).std()\n","      df[f\"{base_col}_diff_{window}\"] = df[base_col].diff(window)\n","      df[f\"{base_col}_pct_{window}\"] = df[base_col].pct_change(window)\n","\n","  df['y_next'] = df['y'].shift(-1)                            # y_next = tomorrow's y (close price)\n","  df.loc[df.index[-1], 'y_next'] = df.loc[df.index[-1], 'y']  # the TARGET cell. y_next tomorrow = y today\n","  df = df.fillna(method='ffill').fillna(method='bfill')\n","\n","  return df\n","\n","df = generate_ts_features(df)\n","df_orig = generate_ts_features(df_orig)\n","\n","# Replace infinite values with NaN in X_train and y_train\n","df = df.replace([np.inf, -np.inf], np.nan)\n","df = df.fillna(method='ffill').fillna(method='bfill')\n","\n","display(df.tail(1))\n","display(df_orig.tail(1))"],"metadata":{"id":"U5zy5Pbx8D4n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X = df.drop(columns=['Date','y_next'])\n","y = df['y_next']\n","\n","X_train, X_test = X.iloc[:-2], X.iloc[-1:]\n","y_train, y_test = y.iloc[:-2], y.iloc[-1:]\n","\n","del X,y\n","\n","X_train.shape, X_test.shape, y_train.shape, y_test.shape"],"metadata":{"id":"sg5V2yZd8JSz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from xgboost import XGBRegressor\n","from sklearn.metrics import mean_absolute_error\n","\n","xgb = XGBRegressor(\n","    n_estimators=300,\n","    max_depth=5,\n","    learning_rate=0.05,\n","    subsample=0.8,\n","    colsample_bytree=0.8,\n","    objective=\"reg:squarederror\")\n","\n","xgb.fit(X_train, y_train)"],"metadata":{"id":"e7RltrgL8RKd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pred = xgb.predict(X_test)\n","pred_orig = y_scaler.inverse_transform(pred.reshape(-1, 1))[0, 0] # Corrected to use NumPy indexing\n","\n","df.loc[df.index[-1], 'y_next'] = pred\n","df_orig.loc[df_orig.index[-1], 'y_next'] = pred_orig\n","\n","print(f\"{project_config['TKL']} {df['Date'].iloc[-1].date()} ${pred_orig:.2f}\")\n","display(df.tail(1))\n","display(df_orig.tail(1))"],"metadata":{"id":"4OFKSP7x8VEp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import xgboost # Import the xgboost module\n","\n","# Plot feature importance based on \"weight\" (number of times a feature appears in a tree)\n","plt.figure(figsize=(10, 6))\n","xgboost.plot_importance(xgb, max_num_features=20) # Pass the xgb regressor object\n","plt.title(\"Feature Importance (Weight)\")\n","plt.show()\n","\n","# 1. Get feature importance by weight\n","importance_dict = xgb.get_booster().get_score(importance_type='weight')\n","\n","# 2. Convert to DataFrame\n","df_importance = pd.DataFrame(list(importance_dict.items()), columns=['Feature', 'Score'])\n","\n","# 3. Sort high → low\n","df_importance = df_importance.sort_values(by='Score', ascending=False)\n","\n","# 4. Compute threshold = 10% of top feature\n","top_score = df_importance['Score'].iloc[0]\n","threshold = top_score * 0.05   # 5%\n","\n","# 5. Select only strong features\n","df_top = df_importance[df_importance['Score'] >= threshold]\n","\n","# 6. Convert to list\n","top_weight_features_list = df_top['Feature'].tolist()[:10:]\n","\n","print(\"Weight Threshold:\", threshold)\n","print(\"Selected Weight Features:\", top_weight_features_list)"],"metadata":{"id":"lMGY5V5c8cbs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# OPTIONAL: Plot based on \"gain\" (average gain of splits which use the feature)\n","# \"Gain\" is often more accurate for finding what actually drives the prediction.\n","plt.figure(figsize=(10, 6))\n","xgboost.plot_importance(xgb, importance_type='gain', max_num_features=20) # Pass the xgb regressor object\n","plt.title(\"Feature Importance (Gain)\")\n","plt.show()\n","\n","# 1. Build importance DataFrame\n","importance_dict = xgb.get_booster().get_score(importance_type='gain')\n","df_importance = pd.DataFrame(list(importance_dict.items()), columns=['Feature', 'Score'])\n","\n","# 2. Sort high to low\n","df_importance = df_importance.sort_values(by='Score', ascending=False)\n","\n","# 3. Compute threshold = 10% of top feature\n","top_score = df_importance['Score'].iloc[0]\n","threshold = top_score * 0.05\n","\n","# 4. Select features with Score >= threshold\n","df_top = df_importance[df_importance['Score'] >= threshold]\n","\n","# 5. Extract feature names\n","top_gain_features_list = df_top['Feature'].tolist()[:10:]\n","\n","print(\"Threshold:\", threshold)\n","print(\"Selected features:\", top_gain_features_list)"],"metadata":{"id":"_moMneSf8ea2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["top_features = list(set(top_gain_features_list) | set(top_weight_features_list))\n","top_features"],"metadata":{"id":"Skemoxht8gml"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["final_df = df[['Date','y_next'] + top_features]\n","\n","for col in top_features + ['y_next']:\n","  final_df[f\"{col}_orig\"] = df_orig[col]\n","\n","display(final_df.tail(1))\n","final_df.info()"],"metadata":{"id":"S1FVpS5W8jNI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_csv_path = f\"{os.getenv('PROJECT_PATH')}{project_config['data_directory']}{project_config['TKL']}.df.csv\"\n","final_df[:-1:].to_csv(df_csv_path)"],"metadata":{"id":"MGIxwWx38lEJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j6X7iNBA80O3"},"source":["# Load dataset file of current TKL"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kZNVRwh78gCz"},"outputs":[],"source":["print(f\"Loading dataset for {project_config['TKL']} .....\\n\")\n","\n","DATASET = f\"{os.getenv('PROJECT_PATH')}{project_config['data_directory']}{project_config['TKL']}.df.csv\"\n","df_all = pd.read_csv(DATASET, index_col=False)\n","df_all = df_all.drop(columns=[\"Unnamed: 0\"])\n","\n","cols_orig = [col for col in df_all.columns if col.endswith('_orig')]\n","cols_normalized = [col for col in df_all.columns if not col.endswith('_orig')]\n","\n","df = df_all[cols_normalized]\n","df_orig = df_all[['Date'] + cols_orig]\n","\n","print(f\"\\ndf for training\")\n","print(f\"-----------------\")\n","display(df.tail(1))\n","\n","print(f\"\\ndf for visualization\")\n","print(f\"----------------------\")\n","display(df_orig.tail(1))"]},{"cell_type":"markdown","metadata":{"id":"zcp4t8WgCz-0"},"source":["# Data prep"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MGNujJS1EEvV"},"outputs":[],"source":["def make_datasets(df, X_cols, y_col):\n","\n","    LOOK_BACK_DAYS = int(project_config['LOOK_BACK_DAYS'])\n","\n","    X, y = [], []\n","\n","    for i in range(LOOK_BACK_DAYS, len(df)):\n","        X.append(df.loc[i-LOOK_BACK_DAYS:i-1, X_cols].values)\n","        y.append(df.loc[i, y_col].values[0])\n","\n","    X = np.array(X, dtype=np.float32)\n","    y = np.array(y, dtype=np.float32)\n","    X = np.reshape(X, (X.shape[0], X.shape[1], X.shape[2]))\n","\n","    split = int(0.8 * len(X))\n","    X_train, X_test = X[:split], X[split:]\n","    y_train, y_test = y[:split], y[split:]\n","\n","    dates = pd.to_datetime(df['Date'])\n","    dates_test = dates[-len(y_test):].values\n","\n","    return X_train, X_test, y_train, y_test, dates_test"]},{"cell_type":"markdown","source":["# Train Predict"],"metadata":{"id":"UHhclNMBITSZ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ANWmCT8mGhci"},"outputs":[],"source":["def train_predict(X_train, X_test, y_train, y_test, features, model_name, epochs=0, batch_size=0):\n","\n","  from tensorflow.keras.models import Sequential, Model\n","  from tensorflow.keras.layers import LSTM, Dense, GRU, Conv1D, MaxPooling1D, Flatten, Dropout, Input, Concatenate\n","\n","  epochs = int(project_config['TRAIN_EPOCS']) if epochs == 0 else epochs\n","  batch_size = int(project_config['TRAIN_BATCH_SIZE']) if batch_size == 0 else batch_size\n","\n","  if model_name == 'Parallel.LSTM.GRU':\n","    input_layer = Input(shape=(X_train.shape[1],X_train.shape[2]))\n","    lstm_branch = LSTM(units=50, return_sequences=False)(input_layer)\n","    gru_branch = GRU(units=50, return_sequences=False)(input_layer)\n","    merged = Concatenate()([lstm_branch, gru_branch])\n","    dropout = Dropout(0.1, name='dropout_layer')(merged)\n","    output_layer = Dense(1, activation='sigmoid')(dropout)\n","    model = Model(inputs=input_layer, outputs=output_layer)\n","\n","  if model_name == 'Cascase.CNN.GRU':\n","    model = Sequential([\n","        Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1],X_train.shape[2])),\n","        MaxPooling1D(pool_size=2),\n","        GRU(50, return_sequences=False),\n","        Dense(25, activation='relu'),\n","        Dense(1)\n","  ])\n","\n","  if model_name == 'Cascase.CNN.LSTM':\n","    model = Sequential([\n","        Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1],X_train.shape[2])),\n","        MaxPooling1D(pool_size=2),\n","        LSTM(50, return_sequences=False),\n","        Dense(25, activation='relu'),\n","        Dense(1)\n","  ])\n","\n","  if model_name == 'CNN':\n","    model = Sequential([\n","        Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1],X_train.shape[2])),\n","        MaxPooling1D(pool_size=2),\n","        Conv1D(filters=128, kernel_size=3, activation='relu'),\n","        MaxPooling1D(pool_size=2),\n","        Flatten(),\n","        Dense(100, activation='relu'),\n","        Dropout(0.2),\n","        Dense(1)\n","  ])\n","\n","  if model_name == 'LSTM':\n","    model = Sequential([\n","        LSTM(50, return_sequences=True, input_shape=(X_train.shape[1],X_train.shape[2])),\n","        LSTM(10),\n","        Dense(1)\n","  ])\n","\n","  if model_name == 'GRU':\n","    model = Sequential([\n","      GRU(50, return_sequences=True, input_shape=(X_train.shape[1],X_train.shape[2])),\n","      GRU(10),\n","      Dense(1)\n","  ])\n","\n","  model.compile(optimizer='adam', loss='mse')\n","\n","  print(\"\\n=====================================================================\\n\")\n","  print(f\"TKL: {project_config['TKL']}\")\n","\n","  print(f\"Model: {model_name}\")\n","  print(f\"Features: {features}\")\n","  model.summary()\n","\n","  history = model.fit(X_train, y_train,\n","                      epochs=int(project_config['TRAIN_EPOCS']),\n","                      batch_size=int(project_config['TRAIN_BATCH_SIZE']),\n","                      validation_data=(X_test, y_test),\n","                      verbose=1)\n","\n","  pred = model.predict(X_test)\n","  unscaled_prediction = y_scaler.inverse_transform(pred.reshape(-1, 1))\n","  unscaled_y_test = y_scaler.inverse_transform(y_test.reshape(-1, 1))\n","\n","  from sklearn.metrics import mean_absolute_error\n","  mae = mean_absolute_error(unscaled_y_test, unscaled_prediction)\n","\n","  from sklearn.metrics import r2_score\n","  r2 = r2_score(unscaled_y_test, unscaled_prediction)\n","\n","  print(f\"TKL: {project_config['TKL']}\")\n","  print(f\"Model: {model_name}\")\n","  print(f\"Features: {features}\")\n","  print(f\"MAE: {mae:.4f}\")\n","  print(f\"R²: {r2:.4f}\")\n","\n","  return unscaled_prediction, unscaled_y_test, model"]},{"cell_type":"markdown","source":["# Racing Models"],"metadata":{"id":"cihXcKfHILkK"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"2cGh7McfuF7p"},"outputs":[],"source":["X_cols_full = [col for col in df.columns if col not in ['Date','y_next']]\n","X_cols_exho = [col for col in df.columns if '_' not in col and col != 'Date']\n","X_cols_tkl  = ['y']\n","y_col       = ['y_next']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jwaRKpLaPW-Y"},"outputs":[],"source":["from sklearn.metrics import mean_absolute_error\n","from sklearn.metrics import r2_score\n","\n","def race_models(tkl, models_to_try):\n","\n","  results_df = pd.DataFrame(columns=[ \"model_name\", \"X_cols\",  \"prediction\", \"mae\", \"r2\", \"model\" ])\n","\n","  for X_cols in [X_cols_full, X_cols_exho, X_cols_tkl]:\n","\n","    X_train, X_test, y_train, y_test, dates_test = make_datasets(df, X_cols, y_col)\n","\n","    if 'Parallel.LSTM.GRU' in models_to_try:\n","      unscaled_prediction, unscaled_y_test, model = train_predict(X_train, X_test, y_train, y_test, X_cols, model_name=\"Parallel.LSTM.GRU\")\n","      results_df.loc[len(results_df)] = {\n","        \"model_name\": \"Parallel.LSTM.GRU\",\n","        \"X_cols\": X_cols,\n","        \"prediction\": unscaled_prediction,\n","        \"mae\": mean_absolute_error(unscaled_y_test, unscaled_prediction),\n","        \"r2\": r2_score(unscaled_y_test, unscaled_prediction),\n","        \"model\": model\n","        }\n","\n","    if 'Cascase.CNN.GRU' in models_to_try:\n","      unscaled_prediction, unscaled_y_test, model = train_predict(X_train, X_test, y_train, y_test, X_cols, model_name=\"Cascase.CNN.GRU\")\n","      results_df.loc[len(results_df)] = {\n","        \"model_name\": \"Cascase.CNN.GRU\",\n","        \"X_cols\": X_cols,\n","        \"prediction\": unscaled_prediction,\n","        \"mae\": mean_absolute_error(unscaled_y_test, unscaled_prediction),\n","        \"r2\": r2_score(unscaled_y_test, unscaled_prediction),\n","        \"model\": model\n","        }\n","\n","    if 'Cascase.CNN.LSTM' in models_to_try:\n","      unscaled_prediction, unscaled_y_test, model = train_predict(X_train, X_test, y_train, y_test, X_cols, model_name=\"Cascase.CNN.LSTM\")\n","      results_df.loc[len(results_df)] = {\n","        \"model_name\": \"Cascase.CNN.LSTM\",\n","        \"X_cols\": X_cols,\n","        \"prediction\": unscaled_prediction,\n","        \"mae\": mean_absolute_error(unscaled_y_test, unscaled_prediction),\n","        \"r2\": r2_score(unscaled_y_test, unscaled_prediction),\n","        \"model\": model\n","        }\n","\n","    if 'CNN' in models_to_try:\n","      unscaled_prediction, unscaled_y_test, model = train_predict(X_train, X_test, y_train, y_test, X_cols, model_name=\"CNN\")\n","      results_df.loc[len(results_df)] = {\n","        \"model_name\": \"CNN\",\n","        \"X_cols\": X_cols,\n","        \"prediction\": unscaled_prediction,\n","        \"mae\": mean_absolute_error(unscaled_y_test, unscaled_prediction),\n","        \"r2\": r2_score(unscaled_y_test, unscaled_prediction),\n","        \"model\": model\n","        }\n","\n","    if 'GRU' in models_to_try:\n","      unscaled_prediction, unscaled_y_test, model = train_predict(X_train, X_test, y_train, y_test, X_cols, model_name=\"GRU\")\n","      results_df.loc[len(results_df)] = {\n","        \"model_name\": \"GRU\",\n","        \"X_cols\": X_cols,\n","        \"prediction\": unscaled_prediction,\n","        \"mae\": mean_absolute_error(unscaled_y_test, unscaled_prediction),\n","        \"r2\": r2_score(unscaled_y_test, unscaled_prediction),\n","        \"model\": model\n","      }\n","\n","    if 'LSTM' in models_to_try:\n","      unscaled_prediction, unscaled_y_test, model = train_predict(X_train, X_test, y_train, y_test, X_cols, model_name=\"LSTM\")\n","      results_df.loc[len(results_df)] = {\n","        \"model_name\": \"LSTM\",\n","        \"X_cols\": X_cols,\n","        \"prediction\": unscaled_prediction,\n","        \"mae\": mean_absolute_error(unscaled_y_test, unscaled_prediction),\n","        \"r2\": r2_score(unscaled_y_test, unscaled_prediction),\n","        \"model\": model\n","      }\n","\n","  return results_df, dates_test, unscaled_y_test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kCf0K2YbX2yl"},"outputs":[],"source":["def plot_results(results_df, dates_test, unscaled_y_test):\n","\n","  import matplotlib.dates as mdates # Added import\n","\n","  for model_name in results_df.model_name.unique():\n","    plt.figure(figsize=(10,6))\n","    plt.title(f\"{project_config['TKL']} {model_name}\")\n","    # Note: unscaled_y_test and dates_test are assumed to be defined in the global scope\n","    # and represent the test data for comparison with all predictions.\n","    plt.plot(dates_test, unscaled_y_test, label=f\"Actual\", linewidth=2, color='black')\n","\n","    # Corrected iteration: iterate over rows using .iterrows()\n","    for index, entry in results_df[results_df.model_name == model_name].iterrows():\n","      plt.plot(dates_test, entry.prediction, label=f\"features: {entry.X_cols}\", linestyle='--')\n","\n","    ax = plt.gca()\n","    ax.xaxis.set_major_locator(mdates.MonthLocator())\n","    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n","\n","    plt.xticks(rotation=45)\n","    plt.tight_layout()\n","\n","    plt.legend()\n","    plt.show()\n","\n","  # Convert lists in 'X_cols' to tuples for unique identification\n","  # This is necessary because lists are unhashable, causing TypeError with .unique()\n","  results_df['X_cols_tuple'] = results_df['X_cols'].apply(tuple)\n","\n","  for features_tuple in results_df.X_cols_tuple.unique():\n","    plt.figure(figsize=(10,6))\n","    # Convert tuple back to list for display purposes in the title\n","    features = list(features_tuple)\n","    plt.title(f\"{project_config['TKL']} {features}\")\n","    # Note: unscaled_y_test and dates_test are assumed to be defined in the global scope\n","    # and represent the test data for comparison with all predictions.\n","    plt.plot(dates_test, unscaled_y_test, label=f\"Actual\", linewidth=2, color='black')\n","\n","    # Corrected iteration: iterate over rows using .iterrows()\n","    for index, entry in results_df[results_df.X_cols_tuple == features_tuple].iterrows():\n","      plt.plot(dates_test, entry.prediction, label=f\":{entry.model_name}\", linestyle='--')\n","\n","    ax = plt.gca()\n","    ax.xaxis.set_major_locator(mdates.MonthLocator())\n","    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n","\n","    plt.xticks(rotation=45)\n","    plt.tight_layout()\n","\n","    plt.legend()\n","    plt.show()"]},{"cell_type":"code","source":["def predict_next_days(winning_model, winning_model_name, winning_model_features, future_days=10):\n","\n","  # Re-create X_train, X_test, y_train, y_test, dates_test with the winning model's features\n","  # This ensures that X_test has the correct number of features for the winning_model.\n","  X_train_winning, X_test_winning, y_train_winning, y_test_winning, dates_test_winning = make_datasets(df, winning_model_features, y_col)\n","\n","  last_historical_block = X_test_winning[-1:] # Select the last sequence from X_test_winning\n","  last_historical_day = df.iloc[-1].Date\n","  future_rolling_block = last_historical_block.copy()\n","  future_predictions = []\n","\n","  for _ in range(future_days):\n","\n","    next_pred_day = winning_model.predict(future_rolling_block)[0]\n","    future_predictions.append(next_pred_day)\n","    # To simulate new data, shift the window and replace the last element with the new prediction\n","    # The new prediction (next_pred_day) represents 'y_next', so it becomes the 'y' feature\n","    # in the next time step's input block. Other features would need to be synthesized or assumed.\n","    # For simplicity and given the model was trained on ['y', 'NASDAQ'] with 'y_next' as target,\n","    # we need to ensure the rolling block maintains the correct feature structure.\n","    # The error suggests a feature mismatch, so for a simple 'y' and 'NASDAQ' model,\n","    # we need to ensure 'future_rolling_block' is updated correctly.\n","\n","    # The original implementation `future_rolling_block[:, -1, 0] = next_pred_day`\n","    # implicitly assumes a single feature is being updated, which is correct for\n","    # a model trained on only ['y']. For a model trained on ['y', 'NASDAQ'],\n","    # if 'y' is the first feature (index 0) and 'NASDAQ' is the second (index 1),\n","    # we can update the 'y' feature and either leave 'NASDAQ' as is or synthesize it.\n","    # Given the problem's focus on `y_next`, it is reasonable to assume that the 'y' feature\n","    # in the input sequence is what `next_pred_day` is meant to update.\n","\n","    # Let's adjust `future_rolling_block` to correctly handle multiple features if the winning model requires it.\n","    # If the winning model uses multiple features, the prediction `next_pred_day` (which is 'y_next')\n","    # corresponds to the 'y' feature for the next time step. The other features ('NASDAQ' in this case)\n","    # would typically need to be forecasted or carried forward.\n","    # For this simple rolling prediction, we'll assume the predicted 'y' replaces the 'y' feature\n","    # in the next input window, and other features remain the same from the last historical block\n","    # or are also predicted/synthesized.\n","\n","    # For a model with ['y', 'NASDAQ'] features, if the input is (batch, timesteps, features):\n","    # future_rolling_block should have shape (1, LOOK_BACK_DAYS, num_features).\n","    # next_pred_day is a single value, representing the predicted 'y_next'.\n","    # We need to update the 'y' feature in the new time step of the rolling block.\n","    # Assuming 'y' is the first feature (index 0) and NASDAQ is the second (index 1).\n","    # The `np.roll` operation shifts the time series data. We then update the last time step's features.\n","\n","    # Create a new row for the next time step, assuming 'y' is the first feature.\n","    # If 'winning_model_features' was more complex and included 'NASDAQ',\n","    # we would need a way to populate the 'NASDAQ' value for the next step.\n","    # For now, let's assume `next_pred_day` updates the *first* feature (index 0) in the last time step\n","    # of the rolling window, and if other features exist, they remain as they were in the previous last time step.\n","\n","    new_input_row = future_rolling_block[0, -1, :].copy() # Copy last row of features\n","    new_input_row[0] = next_pred_day # Update the 'y' feature (first feature)\n","\n","    future_rolling_block = np.roll(future_rolling_block, -1, axis=1) # Shift time window\n","    future_rolling_block[0, -1, :] = new_input_row # Place the new feature vector at the end\n","\n","  # Calculate min and max for 'y_next' from the original unscaled data (df_orig)\n","  # This allows us to manually inverse transform the single predicted value.\n","  # min_y_next_orig = df_orig['y_next_orig'].min()\n","  # max_y_next_orig = df_orig['y_next_orig'].max()\n","  # Inverse transform the scaled prediction using the min-max formula\n","  #np.array(future_predictions) * (max_y_next_orig - min_y_next_orig) + min_y_next_orig\n","  unscaled_future_predictions = X_scaler.inverse_transform(np.array(future_predictions).reshape(-1, 1))\n","\n","  print(f\"Using {winning_model_name} and {winning_model_features} to predicti the next {future_days} days:\")\n","  print(f\"-------------------------------------------\")\n","  for i, p in enumerate(unscaled_future_predictions, start=1):\n","    # Convert last_historical_day to datetime object and add days\n","    prediction_date = pd.to_datetime(last_historical_day) + pd.offsets.BDay(i)\n","    print(f\"{prediction_date.strftime('%Y-%m-%d')}: {float(p):.2f}\")"],"metadata":{"id":"K1Qbdh5p--nO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for stock in project_config['STOCKS']:\n","\n","  project_config['TKL'] = stock\n","\n","  results_df, dates_test, unscaled_y_test = race_models(project_config['TKL'], project_config['RACING_MODELS'].split())\n","  plot_results(results_df, dates_test, unscaled_y_test)\n","\n","  print(f\"AND THE WINNER for {project_config['TKL']} IS ...\")\n","  display(results_df[['model_name', 'X_cols', 'mae', 'r2']].sort_values(by='mae', ascending=True))\n","\n","  winning_model = results_df.sort_values(by='mae', ascending=True).iloc[0].model\n","  winning_model_name = results_df.sort_values(by='mae', ascending=True).iloc[0].model_name\n","  winning_model_features = results_df.sort_values(by='mae', ascending=True).iloc[0].X_cols\n","\n","  predict_next_days(winning_model, winning_model_name, winning_model_features, days=10)\n","\n","  model_path = f\"{os.getenv('PROJECT_PATH')}{project_config['pickles_directory']}{project_config['TKL']}.model.{winning_model_name}.{winning_model_features}.keras\"\n","  df_path = f\"{os.getenv('PROJECT_PATH')}{project_config['pickles_directory']}{project_config['TKL']}.df.pkl\"\n","  df_orig_path = f\"{os.getenv('PROJECT_PATH')}{project_config['pickles_directory']}{project_config['TKL']}.df_orig.pkl\"\n","\n","  winning_model.save(model_path)\n","  df[ ['Date','y_next']+winning_model_features ].to_pickle(df_path)\n","  df[ ['Date','y_next']+winning_model_features ].to_pickle(df_orig_path)"],"metadata":{"id":"-gVNOS9o6kUF"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO51uk+JexSKDXdCgW9Tq0w"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
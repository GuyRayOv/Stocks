{"cells":[{"cell_type":"markdown","metadata":{"id":"4Ns2SNtAqGwM"},"source":["# Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hjNT5fdaAq2u"},"outputs":[],"source":["import sys\n","import os\n","from google.colab import drive\n","from google.colab import files\n","from dotenv import load_dotenv\n","import json\n","import warnings\n","import pickle\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","warnings.filterwarnings(\"ignore\")\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"DRGyC6FrqbZA"},"source":["# Bootstrap"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z1d9o7FkEb0h"},"outputs":[],"source":["np.random.seed(31071967)\n","\n","# Find and load the .env file from the current or parent directories\n","load_dotenv()\n","\n","drive.mount('/content/drive')\n","\n","with open(f\"{os.getenv('PROJECT_PATH')}/src/config.json\", 'r') as f:\n","    project_config = json.load(f)\n","    project_config.pop('_comment', None)\n","    project_config.pop('_note', None)\n","    f.close()"]},{"cell_type":"code","source":["project_config['STOCKS'] = project_config['STOCKS'].split()\n","project_config['TKL'] = project_config['STOCKS'][0]\n","project_config['TKL']"],"metadata":{"id":"U-hkIN07q1Xx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Run prediciton"],"metadata":{"id":"xxUmFdtwoymK"}},{"cell_type":"code","source":["def make_datasets(df, X_cols, y_col):\n","\n","    LOOK_BACK_DAYS = int(project_config['LOOK_BACK_DAYS'])\n","\n","    X, y = [], []\n","\n","    for i in range(LOOK_BACK_DAYS, len(df)):\n","        X.append(df.loc[i-LOOK_BACK_DAYS:i-1, X_cols].values)\n","        y.append(df.loc[i, y_col].values[0])\n","\n","    X = np.array(X, dtype=np.float32)\n","    y = np.array(y, dtype=np.float32)\n","    X = np.reshape(X, (X.shape[0], X.shape[1], X.shape[2]))\n","\n","    split = int(0.8 * len(X))\n","    X_train, X_test = X[:split], X[split:]\n","    y_train, y_test = y[:split], y[split:]\n","\n","    dates = pd.to_datetime(df['Date'])\n","    dates_test = dates[-len(y_test):].values\n","\n","    return X_train, X_test, y_train, y_test, dates_test"],"metadata":{"id":"ONrCZ76apDKg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def predict_next_days(df, model, model_name, model_features, future_days=10):\n","\n","  X_train, X_test, y_train, y_test, dates_test = make_datasets(df, model_features, 'y_next')\n","\n","  last_historical_block = X_test[-1:] # Select the last sequence from X_test\n","  last_historical_day = df.iloc[-1].Date\n","  future_rolling_block = last_historical_block.copy()\n","  future_predictions = []\n","\n","  for _ in range(future_days):\n","\n","    next_pred_day = model.predict(future_rolling_block)[0]\n","    future_predictions.append(next_pred_day)\n","\n","    new_input_row = future_rolling_block[0, -1, :].copy() # Copy last row of features\n","    new_input_row[0] = next_pred_day # Update the 'y' feature (first feature)\n","    future_rolling_block = np.roll(future_rolling_block, -1, axis=1) # Shift time window\n","    future_rolling_block[0, -1, :] = new_input_row # Place the new feature vector at the end\n","\n","    min_y_next_orig = df_orig['y_next_orig'].min()\n","    max_y_next_orig = df_orig['y_next_orig'].max()\n","    np.array(future_predictions) * (max_y_next_orig - min_y_next_orig) + min_y_next_orig\n","\n","  #print(f\"Using {winning_model_name} and {winning_model_features} to predicti the next {future_days} days:\")\n","  #print(f\"-------------------------------------------\")\n","  for i, p in enumerate(unscaled_future_predictions, start=1):\n","    # Convert last_historical_day to datetime object and add days\n","    prediction_date = pd.to_datetime(last_historical_day) + pd.offsets.BDay(i)\n","    print(f\"{prediction_date.strftime('%Y-%m-%d')}: {float(p):.2f}\")"],"metadata":{"id":"phnIIbwEnrtW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def make_datasets(df, X_cols, y_col):\n","\n","    LOOK_BACK_DAYS = int(project_config['LOOK_BACK_DAYS'])\n","\n","    X, y = [], []\n","\n","    for i in range(LOOK_BACK_DAYS, len(df)):\n","        X.append(df.loc[i-LOOK_BACK_DAYS:i-1, X_cols].values)\n","        y.append(df.loc[i, y_col]) # Corrected line\n","\n","    X = np.array(X, dtype=np.float32)\n","    y = np.array(y, dtype=np.float32)\n","    X = np.reshape(X, (X.shape[0], X.shape[1], X.shape[2]))\n","\n","    split = int(0.8 * len(X))\n","    X_train, X_test = X[:split], X[split:]\n","    y_train, y_test = y[:split], y[split:]\n","\n","    dates = pd.to_datetime(df['Date'])\n","    dates_test = dates[-len(y_test):].values\n","\n","    return X_train, X_test, y_train, y_test, dates_test\n","\n","def predict_next_days(df, model, model_name, model_features, future_days=10):\n","\n","  X_train, X_test, y_train, y_test, dates_test = make_datasets(df, model_features, 'y_next')\n","\n","  last_historical_block = X_test[-1:] # Select the last sequence from X_test\n","  last_historical_day = df.iloc[-1].Date\n","  future_rolling_block = last_historical_block.copy()\n","  future_predictions = []\n","\n","  for _ in range(future_days):\n","\n","    next_pred_day = model.predict(future_rolling_block)[0]\n","    future_predictions.append(next_pred_day)\n","\n","    new_input_row = future_rolling_block[0, -1, :].copy() # Copy last row of features\n","    new_input_row[0] = next_pred_day # Update the 'y' feature (first feature)\n","    future_rolling_block = np.roll(future_rolling_block, -1, axis=1) # Shift time window\n","    future_rolling_block[0, -1, :] = new_input_row # Place the new feature vector at the end\n","\n","    min_y_next_orig = df_orig['y_next_orig'].min()\n","    max_y_next_orig = df_orig['y_next_orig'].max()\n","    # Re-scale the predicted values\n","    unscaled_future_predictions = np.array(future_predictions) * (max_y_next_orig - min_y_next_orig) + min_y_next_orig\n","\n","  #print(f\"Using {winning_model_name} and {winning_model_features} to predicti the next {future_days} days:\")\n","  #print(f\"-------------------------------------------\")\n","  for i, p in enumerate(unscaled_future_predictions, start=1):\n","    # Convert last_historical_day to datetime object and add days\n","    prediction_date = pd.to_datetime(last_historical_day) + pd.offsets.BDay(i)\n","    print(f\"{prediction_date.strftime('%Y-%m-%d')}: {float(p):.2f}\")\n","\n","\n","from pathlib import Path\n","\n","pickles_path = Path(f\"{os.getenv('PROJECT_PATH')}{project_config['pickles_directory']}\")\n","folder = Path(pickles_path)\n","\n","\n","if list(folder.glob(f\"{project_config['TKL']}.model*.keras\")) != []:\n","\n","  import ast\n","  model_path = list(folder.glob(f\"{project_config['TKL']}.model*.keras\"))[0]\n","  df_path = list(folder.glob(f\"{project_config['TKL']}.df.pkl\"))[0]\n","  df_orig_path = list(folder.glob(f\"{project_config['TKL']}.df_orig.pkl\"))[0]\n","\n","  fname = model_path.name  # extract filename only\n","  base = fname.removesuffix(\".keras\")\n","  tkl_name, _, best_model_name, features_str = base.split(\".\", maxsplit=3)\n","  best_model_features = ast.literal_eval(features_str)\n","\n","  print(f\"TKL: {tkl_name}\")\n","  print(f\"Model: {best_model_name}\")\n","  print(f\"Features: {best_model_features}\")\n","\n","  import tensorflow as tf\n","  df = pd.read_pickle(df_path)\n","  df_orig = pd.read_pickle(df_orig_path)\n","  best_model = tf.keras.models.load_model(model_path)\n","\n","  display(df.tail(1), df_orig.tail(1))\n","  best_model.summary()\n","\n","  predict_next_days(df, best_model, best_model_name, best_model_features, future_days=10)"],"metadata":{"id":"_bIc0ok3wTVt"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMgpQzU/IGvs7V4MO2aGoUY"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
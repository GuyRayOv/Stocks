{"cells":[{"cell_type":"markdown","source":["# Imports"],"metadata":{"id":"vy7LcUPlsK0X"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"FbWvCAKmUQD6","executionInfo":{"status":"ok","timestamp":1763716847172,"user_tz":-120,"elapsed":548,"user":{"displayName":"Guy Ray","userId":"03709370811123485399"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from google.colab import files\n","import os\n","import warnings\n","import pickle\n","\n","warnings.filterwarnings(\"ignore\")\n","%matplotlib inline\n","\n","np.random.seed(31071967)"]},{"cell_type":"markdown","source":["# Bootstrap"],"metadata":{"id":"zdP_xX80sN98"}},{"cell_type":"code","source":["import sys\n","import os\n","from dotenv import load_dotenv\n","\n","# Find and load the .env file from the current or parent directories\n","load_dotenv()\n","\n","# Access the environment variable using os.getenv()\n","PROJECT_PATH = os.getenv('PROJECT_PATH')\n","CONFIG_FILE = f\"{PROJECT_PATH}/src/config.json\""],"metadata":{"id":"Gch0QCmZIG5C","executionInfo":{"status":"ok","timestamp":1763716847176,"user_tz":-120,"elapsed":2,"user":{"displayName":"Guy Ray","userId":"03709370811123485399"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","import json\n","\n","drive.mount('/content/drive')\n","\n","with open(CONFIG_FILE, 'r') as f:\n","    project_config = json.load(f)\n","    project_config.pop('_comment', None)\n","    project_config.pop('_note', None)\n","    f.close()"],"metadata":{"id":"jsWS0uXZwymE","colab":{"base_uri":"https://localhost:8080/","height":228},"executionInfo":{"status":"error","timestamp":1763716879782,"user_tz":-120,"elapsed":32605,"user":{"displayName":"Guy Ray","userId":"03709370811123485399"}},"outputId":"3c76eaa8-a5c6-4868-8822-da2ebcc9cab4"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'None/src/config.json'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-270399383.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONFIG_FILE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mproject_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mproject_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_comment'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'None/src/config.json'"]}]},{"cell_type":"markdown","metadata":{"id":"I3b2TInfHPW_"},"source":["#[Download dataset from Kaggle](https://www.kaggle.com/datasets/joebeachcapital/30000-spotify-songs?select=spotify_songs.csv)"]},{"cell_type":"code","source":["import os\n","import json\n","import subprocess\n","\n","KAGGLE_DATAFILE=\"/content/spotify_data/spotify_songs.csv\"\n","\n","# STEP 1: check if dataset exists\n","if not os.path.exists(KAGGLE_DATAFILE):\n","    print(\"Downloading Spotify dataset from Kaggle...\")\n","\n","    # STEP 2: create API key file ~/.kaggle/kaggle.json\n","    subprocess.run([\"pip\", \"install\", \"-q\", \"kaggle\"])\n","    os.makedirs(os.path.expanduser(\"~/.kaggle\"), exist_ok=True)\n","\n","    kaggle_config = {\n","        \"username\": os.getenv('KAGGLE_USERNAME'),\n","        \"key\": os.getenv('KAGGLE_API_KEY')\n","    }\n","    kaggle_json_path = os.path.expanduser(\"~/.kaggle/kaggle.json\")\n","    with open(kaggle_json_path, \"w\") as f:\n","        json.dump(kaggle_config, f)\n","    os.chmod(kaggle_json_path, 0o600)\n","\n","    # STEP 3: download dataset\n","    subprocess.run([\"kaggle\", \"datasets\", \"download\", \"-d\", \"joebeachcapital/30000-spotify-songs\"])\n","\n","    # STEP 4: unzip the dataset\n","    subprocess.run([\"unzip\", \"-o\", \"30000-spotify-songs.zip\", \"-d\", \"spotify_data\"])\n","\n","    # STEP 5: list the files\n","    subprocess.run([\"ls\", \"spotify_data\"])\n","\n","    # STEP 6: remove the zip file\n","    os.remove(\"30000-spotify-songs.zip\")\n","\n","else:\n","    print(\"Spotify dataset already exists.\")"],"metadata":{"id":"b1_o-jKN8bZQ","executionInfo":{"status":"aborted","timestamp":1763716879866,"user_tz":-120,"elapsed":105,"user":{"displayName":"Guy Ray","userId":"03709370811123485399"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ndergfMCVUU2"},"source":["#Upload dataset with Pandas"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dS5smbVvVIFP","executionInfo":{"status":"aborted","timestamp":1763716879869,"user_tz":-120,"elapsed":92,"user":{"displayName":"Guy Ray","userId":"03709370811123485399"}}},"outputs":[],"source":["import pandas as pd\n","df = pd.read_csv(KAGGLE_DATAFILE)\n","display(df.head(1)), display(df.tail(1))"]},{"cell_type":"code","source":["# # appearnatly, five tracks appaers twice (different track_ids), each time in a different playlist. Even popularity is not identical.\n","# # decition: keep both copies\n","# duplicates = (df.groupby(['track_name', 'track_artist', 'track_album_id']).filter(lambda x: x['track_id'].nunique() > 1))\n","# duplicates = duplicates.sort_values(['track_artist', 'track_name', 'track_album_id'])"],"metadata":{"id":"0ccOT1t6DM6m","executionInfo":{"status":"aborted","timestamp":1763716879870,"user_tz":-120,"elapsed":19,"user":{"displayName":"Guy Ray","userId":"03709370811123485399"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dBTQ-sneY2P7","executionInfo":{"status":"aborted","timestamp":1763716879872,"user_tz":-120,"elapsed":20,"user":{"displayName":"Guy Ray","userId":"03709370811123485399"}}},"outputs":[],"source":["unique_id_cols = ['track_name']\n","dup_cols       = ['track_album_name', 'playlist_name','playlist_genre','instrumentalness']\n","large_cat_cols = ['track_artist', 'track_album_id', 'playlist_id']\n","small_cat_cols = ['mode','key', 'playlist_subgenre']\n","cont_cols      = ['acousticness', 'danceability','duration_ms','energy', 'liveness', 'loudness', 'speechiness', 'tempo','valence']\n","date_cols      = ['release_date']\n","cat_cols       = large_cat_cols + small_cat_cols\n","y_col          = 'track_popularity'\n","MERGE_ON_COL   = 'track_id' # if we'll need to merge pickels later"]},{"cell_type":"markdown","metadata":{"id":"oQQ4OL_-Me_c"},"source":["#Flatfile"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rBQvB6-NMcB6","executionInfo":{"status":"aborted","timestamp":1763716879873,"user_tz":-120,"elapsed":6,"user":{"displayName":"Guy Ray","userId":"03709370811123485399"}}},"outputs":[],"source":["import numpy as np\n","\n","def write_flat_file(df, filename):\n","  writer = pd.ExcelWriter(filename, engine=\"openpyxl\")\n","\n","  df.head().to_excel(writer, sheet_name='head')\n","  df.tail().to_excel(writer, sheet_name='tail')\n","  df.describe().to_excel(writer, sheet_name='describe')\n","  df.dtypes.to_excel(writer, sheet_name='data_type')\n","  df.select_dtypes(include=np.number).max().to_excel(writer, sheet_name='max - numeric columns')\n","  df.select_dtypes(include=np.number).min().to_excel(writer, sheet_name='min - numeric columns')\n","  df.isnull().sum(axis=0).to_excel(writer, sheet_name='NA')\n","  df.nunique().to_excel(writer, sheet_name='unique')\n","\n","  writer.close()\n","  print(f\"Flat file: {filename}\")\n","\n","flat_file = \"spotify_flatfile.xlsx\"\n","write_flat_file(df, f\"{PROJECT_PATH}{project_config['pickles_directory']}{flat_file}\")"]},{"cell_type":"code","source":["if project_config['split_df'] == '1':\n","  from sklearn.model_selection import train_test_split\n","\n","  # split to avoid data leakage\n","  train_df, test_df = train_test_split(df, test_size=0.3, random_state=42)\n","\n","  # keep the name df for code compliaency\n","  df = train_df\n","  del train_df\n","\n","  # just a mark\n","  df['set']  = 'train'\n","  test_df['set']  = 'test'\n","\n","else:\n","  test_df = None"],"metadata":{"id":"ui-Suadua6AR","executionInfo":{"status":"aborted","timestamp":1763716879874,"user_tz":-120,"elapsed":6,"user":{"displayName":"Guy Ray","userId":"03709370811123485399"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fDhm0eKM7Ugc","executionInfo":{"status":"aborted","timestamp":1763716879876,"user_tz":-120,"elapsed":33406,"user":{"displayName":"Guy Ray","userId":"03709370811123485399"}}},"outputs":[],"source":["def pickle_col(df, col='all', drop_col=False, include_merge_ID=True, pickle_name=\"\"):\n","\n","  import pickle\n","\n","  if pickle_name == \"\":\n","    file_name = f\"{PROJECT_PATH}{project_config['pickles_directory']}{col}.pkl\"\n","  else:\n","    file_name = f\"{PROJECT_PATH}{project_config['pickles_directory']}{pickle_name}.pkl\"\n","\n","  with open(file_name, 'wb') as f:\n","\n","    if col =='all':\n","      pickle.dump(df, f)\n","\n","    elif col in df.columns: # in case we aready droped the col before\n","\n","      # track_id for a later merge, if we need.\n","      #and y_col so can can invetigate the pickel later indepandantly from the main df\n","      pickle.dump(df[[MERGE_ON_COL, col, y_col]], f)\n","\n","    f.close()\n","\n","    if drop_col == True:\n","      df.drop(col, axis=1, inplace=True, errors='ignore')\n","\n","  if project_config['split_df'] == '1':\n","    with open(file_name+\".test.pkl\", 'wb') as f:\n","\n","      if col =='all':\n","        pickle.dump(test_df, f)\n","\n","      elif col in test_df.columns: # in case we aready droped the col before\n","        pickle.dump(test_df[[MERGE_ON_COL, col, y_col]], f)\n","\n","      f.close()\n","\n","    if drop_col == True:\n","      test_df.drop(col, axis=1, inplace=True, errors='ignore')\n","\n","  return df"]},{"cell_type":"code","source":["def apply_cat_mean_as_target_encode(train_df, cat_cols, encode_cols, target_col, test_df=None):\n","\n","  for i in range(len(cat_cols)):\n","    train_means = train_df.groupby(cat_cols[i])[target_col].mean()\n","    train_df[encode_cols[i]] = train_df[cat_cols[i]].map(train_means).astype(float)\n","\n","    if test_df is not None:\n","        test_means = test_df.groupby(cat_cols[i])[target_col].mean()\n","        test_df[encode_cols[i]] = test_df[cat_cols[i]].map(test_means).astype(float)\n","\n","      #test_df[encode_cols[i]] = test_df[cat_cols[i]].map(train_means).astype(float)\n","      #test_df[encode_cols[i]].fillna(train_means, inplace=True)"],"metadata":{"id":"Bt1m7SvUrvXZ","executionInfo":{"status":"aborted","timestamp":1763716879877,"user_tz":-120,"elapsed":33407,"user":{"displayName":"Guy Ray","userId":"03709370811123485399"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9ABpi5ChUwOJ"},"source":["# Dates"]},{"cell_type":"code","source":["#track_album_release_date\n","if \"track_album_release_date\" in df.columns: # maybe I'm just rerunnig this cell\n","\n","\n","  df.track_album_release_date = pd.to_datetime(df.track_album_release_date, format='mixed')\n","  df['release_month' ] = df['track_album_release_date'].dt.month.astype(int)\n","  df['release_year'  ] = df['track_album_release_date'].dt.year.astype(int)\n","  df['release_decade'] = ((df['release_year'] // 10) * 10).astype(int)\n","\n","  if project_config['split_df'] == '1':\n","\n","    test_df.track_album_release_date = pd.to_datetime(test_df.track_album_release_date, format='mixed')\n","    test_df['release_month' ] = test_df['track_album_release_date'].dt.month.astype(int)\n","    test_df['release_year'  ] = test_df['track_album_release_date'].dt.year.astype(int)\n","    test_df['release_decade'] = ((test_df['release_year'] // 10) * 10).astype(int)\n","\n","  apply_cat_mean_as_target_encode(\n","      df,\n","      cat_cols=['release_decade', 'release_month','release_year'],\n","      encode_cols=['release_decade_mean_popularity','release_month_mean_popularity','release_year_mean_popularity'],\n","      target_col='track_popularity',\n","      test_df=test_df)\n","\n","  pickle_col(df, col=\"track_album_release_date\", drop_col=True)"],"metadata":{"id":"TWD3fduP0vwI","executionInfo":{"status":"aborted","timestamp":1763716879877,"user_tz":-120,"elapsed":33407,"user":{"displayName":"Guy Ray","userId":"03709370811123485399"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"38mgBqol6ZY9","executionInfo":{"status":"aborted","timestamp":1763716879878,"user_tz":-120,"elapsed":33405,"user":{"displayName":"Guy Ray","userId":"03709370811123485399"}}},"outputs":[],"source":["plot_columns = 3\n","plot_rows = 2\n","\n","fig = plt.figure(figsize=(12,12))\n","plt.subplots_adjust(hspace=1.0)\n","sb.set(font_scale=0.8)\n","\n","for plot_counter, col in enumerate(['release_decade','release_year', 'release_month'],start=1):\n","    plt.subplot(plot_rows, plot_columns, plot_counter)\n","    sb.histplot(x=col, data=df)\n","plt.show()\n","\n","fig = plt.figure(figsize=(12,12))\n","plt.subplots_adjust(hspace=1.0)\n","sb.set(font_scale=0.8)\n","\n","for plot_counter, col in enumerate(['release_decade','release_year', 'release_month'],start=1):\n","    plt.subplot(plot_rows, plot_columns, plot_counter)\n","    sb.barplot(x=col, y=f\"{col}_mean_popularity\", data=df)\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"OHjp-_R-USCt"},"source":["# Categories\n","\n","Uniqe IDs: **drop** to a pickel. Keeping track_id col with each pickel for possible merges later\n","\n","Duplications (album_name, album_id, genre): **drop** to a pickel\n","\n","Lerge cat cols: **replace** with the 'mean' popularity of each category - making it a continuence variabe on\n","\n","Small cat cols: **group** categories based on ANOVA-tests between mean popoularity of each cat. **Replace** with with the 'mean' popularity of each category."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gxxTc43d2a1e","executionInfo":{"status":"aborted","timestamp":1763716879899,"user_tz":-120,"elapsed":33425,"user":{"displayName":"Guy Ray","userId":"03709370811123485399"}}},"outputs":[],"source":["# unique ids and duplicaitons: drop\n","for col in unique_id_cols + dup_cols:\n","  df = pickle_col(df, col=col, drop_col=True)"]},{"cell_type":"code","source":["# musical mode\n","df['mode'] = df['mode'].map({0: \"Minor\", 1: \"Major\"}).astype('string')\n","\n","if project_config['split_df'] == '1':\n","  test_df['mode'] = test_df['mode'].map({0: \"Minor\", 1: \"Major\"}).astype('string')"],"metadata":{"id":"nIgnwu0W9hyO","executionInfo":{"status":"aborted","timestamp":1763716879900,"user_tz":-120,"elapsed":33425,"user":{"displayName":"Guy Ray","userId":"03709370811123485399"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vdmXvQ8rr-a9","executionInfo":{"status":"aborted","timestamp":1763716879901,"user_tz":-120,"elapsed":33426,"user":{"displayName":"Guy Ray","userId":"03709370811123485399"}}},"outputs":[],"source":["# 12 categories for musical keys is small enoght to keep\n","# Mapping key-numbers into musical symbols, simpply for a better visualizalizaiton\n","\n","musical_key_dict = {\n","    0: \"C\",\n","    1: \"C♯/D♭\",\n","    2: \"D\",\n","    3: \"D♯/E♭\",\n","    4: \"E\",\n","    5: \"F\",\n","    6: \"F♯/G♭\",\n","    7: \"G\",\n","    8: \"G♯/A♭\",\n","    9: \"A\",\n","    10: \"A♯/B♭\",\n","    11: \"B\"\n","}\n","if df['key'].dtype == 'int64': # or maybe I'm just rerunnig this cell\n","\n","  df['key'] = df['key'].map(musical_key_dict).astype('string')\n","\n","  if project_config['split_df'] == '1':\n","    test_df['key'] = test_df['key'].map(musical_key_dict).astype('string')\n"]},{"cell_type":"code","source":["# using ANOVA for grouping 24 musical genres into 9\n","\n","genre_map = {\n","\n","    'post-teen pop'     : 'Teen Pop',\n","    'permanent wave'    : 'New Wave',\n","\n","    'hip pop'           : 'Mainstream Pop & Hip-Hop',\n","    'hip hop'           : 'Mainstream Pop & Hip-Hop',\n","    'dance pop'         : 'Mainstream Pop & Hip-Hop',\n","    'reggaeton'         : 'Mainstream Pop & Hip-Hop',\n","\n","    'latin pop'         : 'Latin & Urban Trap',\n","    'urban contemporary': 'Latin & Urban Trap',\n","    'trap'              : 'Latin & Urban Trap',\n","\n","    'pop edm'           : 'Electro/Latin Pop Fusion',\n","    'latin hip hop'     : 'Electro/Latin Pop Fusion',\n","    'tropical'          : 'Electro/Latin Pop Fusion',\n","    'electropop'        : 'Electro/Latin Pop Fusion',\n","    'indie poptimism'   : 'Electro/Latin Pop Fusion',\n","\n","    'classic rock'      : 'Classic Rock',\n","    'album rock'        : 'Classic Rock',\n","\n","    'southern hip hop'  : 'Hardcore Hip-Hop & Rock/EDM',\n","    'hard rock'         : 'Hardcore Hip-Hop & Rock/EDM',\n","    'electro house'     : 'Hardcore Hip-Hop & Rock/EDM',\n","    'gangster rap'      : 'Hardcore Hip-Hop & Rock/EDM',\n","\n","    'neo soul'          : 'Soul & Big Room EDM',\n","    'big room'          : 'Soul & Big Room EDM',\n","\n","    'new jack swing'    : 'Retro Swing & Prog House',\n","    'progressive electro house' : 'Retro Swing & Prog House'\n","}\n","\n","df['playlist_genre_grouped'] = df['playlist_subgenre'].map(genre_map).astype('string')\n","\n","if project_config['split_df'] == '1':\n","  test_df['playlist_genre_grouped'] = test_df['playlist_subgenre'].map(genre_map).astype('string')\n","\n","small_cat_cols.append('playlist_genre_grouped')\n","cat_cols.append('playlist_genre_grouped')\n","\n","small_cat_cols.remove('playlist_subgenre')\n","cat_cols.remove('playlist_subgenre')\n","pickle_col(df, col='playlist_subgenre', drop_col=True)"],"metadata":{"id":"90wk52zldiRE","executionInfo":{"status":"aborted","timestamp":1763716879902,"user_tz":-120,"elapsed":33424,"user":{"displayName":"Guy Ray","userId":"03709370811123485399"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yyIbgqv4gQiK","executionInfo":{"status":"aborted","timestamp":1763716879903,"user_tz":-120,"elapsed":33424,"user":{"displayName":"Guy Ray","userId":"03709370811123485399"}}},"outputs":[],"source":["# target encoding: replace categories with mean_popularity of each category\n","for col in cat_cols:\n","\n","  if col in df.columns: # maybe I'm just reruning the cell\n","\n","    apply_cat_mean_as_target_encode(\n","        df,\n","        cat_cols=[col],\n","        encode_cols=[f'{col}_mean_popularity'],\n","        target_col='track_popularity',\n","        test_df=test_df)\n","\n","#keep original col for visuaizations later, it will not be used for the train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0k0Pq5ElZmLH","executionInfo":{"status":"aborted","timestamp":1763716879904,"user_tz":-120,"elapsed":33422,"user":{"displayName":"Guy Ray","userId":"03709370811123485399"}}},"outputs":[],"source":["# plot categories mean popularity\n","\n","plot_columns = 3\n","n_plot = len(large_cat_cols) + len(small_cat_cols) + len(cat_cols)\n","plot_rows = n_plot//plot_columns + n_plot%plot_columns\n","\n","fig = plt.figure(figsize=(20,28))\n","plt.subplots_adjust(hspace=1.0)\n","sb.set(font_scale=0.8)\n","\n","for plot_counter, col in enumerate(large_cat_cols, start=1):\n","  plt.subplot(plot_rows, plot_columns, plot_counter)\n","  sb.histplot(x=f\"{col}_mean_popularity\", data=df, kde=True, bins=15)\n","  # pickle_col(df, col=col, drop_col=True)  # we dont need the original col anymore\n","\n","for plot_counter, col in enumerate(small_cat_cols, start=1):\n","    plt.subplot(plot_rows, plot_columns, len(large_cat_cols) + plot_counter)\n","    order = df.groupby(col)[f\"{col}_mean_popularity\"].mean().sort_values(ascending=False).index\n","    sb.barplot(x=col, y=f\"{col}_mean_popularity\", data=df, order=order)\n","    plt.xticks(rotation=85)\n","    #keep orirignal cols for eda visualzations\n","\n","for plot_counter, col in enumerate(small_cat_cols, start=1):\n","    plt.subplot(plot_rows, plot_columns, len(large_cat_cols) + len(small_cat_cols) + plot_counter)\n","    sb.countplot(x=col, data=df)\n","    plt.xticks(rotation=85)\n","    #keep orirignal cols for eda visualzations\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"x6oXKYlHpvxo"},"source":["# Continuance"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uTCQkFpAp4ms","executionInfo":{"status":"aborted","timestamp":1763716879905,"user_tz":-120,"elapsed":33420,"user":{"displayName":"Guy Ray","userId":"03709370811123485399"}}},"outputs":[],"source":["# plot values\n","plot_columns = 3\n","plot_rows = (len(cont_cols)//plot_columns) + len(cont_cols)%plot_columns\n","\n","fig = plt.figure(figsize=(20,15))\n","plt.subplots_adjust(hspace=0.8)\n","sb.set(font_scale=1.2)\n","\n","for plot_counter, col in enumerate(cont_cols, start=1):\n","    plt.subplot(plot_rows, plot_columns, plot_counter)\n","    sb.histplot(x=col, data=df, kde=True)\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"P1EKeoV39She"},"source":["# Pickle"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XcaTZ1859XTf","executionInfo":{"status":"aborted","timestamp":1763716879906,"user_tz":-120,"elapsed":33420,"user":{"displayName":"Guy Ray","userId":"03709370811123485399"}}},"outputs":[],"source":["from google.colab import files\n","import os\n","\n","with open(f\"{PROJECT_PATH}{project_config['pickles_directory']}{project_config['pickle1']}\", 'wb') as f:\n","  pickle.dump(df, f)\n","  f.close()\n","\n","if project_config['split_df'] == '1':\n","  with open(f\"{PROJECT_PATH}{project_config['pickles_directory']}{project_config['pickle1_test']}\", 'wb') as f:\n","    pickle.dump(test_df, f)\n","    f.close()"]}],"metadata":{"colab":{"provenance":[],"mount_file_id":"1z_z6aKOseMCo3v_YjFnMhl-K4gXimAMR","authorship_tag":"ABX9TyP2QXyZw+M6g9XibmPaSbyR"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}